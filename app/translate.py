import os
import time
import torch
import json
import logging
import aiofiles
import asyncio
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
from concurrent.futures import ThreadPoolExecutor
import GPUtil

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
    handlers=[logging.FileHandler("translation.log"), logging.StreamHandler()],
)
logger = logging.getLogger()

INPUT_CSV = "/home/aruzhan/products_202503121705.csv"
OUTPUT_CSV = "/home/aruzhan/translated_products.csv"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if torch.cuda.is_available():
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(0)}")
    torch.cuda.empty_cache()  # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
else:
    logger.warning("GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU.")

logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
MODEL_PATH = "/models/t5_translate_model"
tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)
logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

BATCH_SIZE = 150  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ

def get_dynamic_threads():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ GPU"""
    try:
        gpus = GPUtil.getGPUs()
        if not gpus:
            return 10  # –ï—Å–ª–∏ GPU –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç

        load = gpus[0].load  # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å GPU (0.0 - 1.0)
        logger.info(f"üéõ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å GPU: {load * 100:.2f}%")

        if load > 0.8:
            return 10  # –ï—Å–ª–∏ GPU —Å–∏–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, —É–º–µ–Ω—å—à–∞–µ–º –¥–æ 10 –ø–æ—Ç–æ–∫–æ–≤
        return 20  # –ï—Å–ª–∏ GPU –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º 20 –ø–æ—Ç–æ–∫–æ–≤
    except Exception as e:
        logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ GPU: {e}")
        return 10  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ —Å—Ç–∞–≤–∏–º 10 –ø–æ—Ç–æ–∫–æ–≤

NUM_THREADS = get_dynamic_threads()
semaphore = asyncio.Semaphore(NUM_THREADS)  # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á

async def write_to_csv(rows):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –≤ CSV."""
    async with aiofiles.open(OUTPUT_CSV, mode="a", encoding="utf-8") as f:
        await f.writelines([",".join(map(str, row)) + "\n" for row in rows])

def parse_json_safe(x):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–±–æ—Ä JSON."""
    try:
        return json.loads(x) if isinstance(x, str) and x.startswith("{") else {}
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ JSON: {e} –≤ —Å—Ç—Ä–æ–∫–µ: {x}")
        return {}

def translate_text(text):
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –û–î–ò–ù —Ç–µ–∫—Å—Ç (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ) –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
    if not text or pd.isna(text):
        return ""

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=256).to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=256,
            no_repeat_ngram_size=3,
            repetition_penalty=1.2,
            length_penalty=1.1,
            num_beams=5,
            early_stopping=True,
        )

    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)

    logger.info(f" –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: \"{text}\" ‚Üí \"{translated_text}\"")
    return translated_text

async def translate_batch(batch):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ë–ê–¢–ß –∏–∑ 150 —Ç–æ–≤–∞—Ä–æ–≤."""
    logger.info(f"–ü–µ—Ä–µ–≤–æ–¥–∏–º {len(batch)} —Ç–æ–≤–∞—Ä–æ–≤...")

    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        translations = await asyncio.gather(*[loop.run_in_executor(executor, translate_text, row["en"]) for _, row in batch.iterrows()])

    return translations

async def process_batch(batch):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á —Ç–æ–≤–∞—Ä–æ–≤: –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ CSV —Å –ª–æ–≥–∞–º–∏ –ø–µ—Ä–µ–≤–æ–¥–∞."""
    async with semaphore:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —á–∏—Å–ª–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
        start_time = time.time()
        translations = await translate_batch(batch)

        csv_rows = []
        for i, (_, row) in enumerate(batch.iterrows()):
            translated_text = translations[i]
            logger.info(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ ID {row['id']}: \"{row['en']}\" ‚Üí \"{translated_text}\"")  # –õ–æ–≥ –ø–µ—Ä–µ–≤–æ–¥–∞
            csv_rows.append([row["id"], row["en"], translated_text, row["product_id"], row["category_id"]])

        await write_to_csv(csv_rows)

        elapsed_time = time.time() - start_time
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(batch)} —Å—Ç—Ä–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫.")

async def process_csv():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç CSV, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—è –±–∞—Ç—á–∏ –Ω–∞ 10-20 –ø–æ—Ç–æ–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ GPU."""
    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")

    try:
        df = await asyncio.to_thread(pd.read_csv, INPUT_CSV, delimiter=";", quotechar='"', on_bad_lines="skip", dtype=str)
        logger.info(f" CSV –∑–∞–≥—Ä—É–∂–µ–Ω, —Å—Ç—Ä–æ–∫: {len(df)}")
    except Exception as e:
        logger.error(f" –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {e}")
        return None

    if "id" not in df.columns or "names" not in df.columns:
        logger.error(" CSV –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã 'id' –∏ 'names'.")
        return None

    df.dropna(subset=["id", "names"], inplace=True)
    logger.info(f" –û—á–∏—â–µ–Ω–Ω—ã–π CSV: {len(df)} —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.")

    df["names"] = df["names"].apply(parse_json_safe)
    df["en"] = df["names"].apply(lambda x: x.get("en", "") if isinstance(x, dict) else "")

    if not os.path.exists(OUTPUT_CSV):
        async with aiofiles.open(OUTPUT_CSV, mode="w", encoding="utf-8") as f:
            await f.write("id,en_name,ru_name,product_id,category_id\n")

    batches = [df.iloc[i:i + BATCH_SIZE] for i in range(0, len(df), BATCH_SIZE)]
    logger.info(f" –í—Å–µ–≥–æ {len(batches)} –±–∞—Ç—á–µ–π –ø–æ {BATCH_SIZE} —Ç–æ–≤–∞—Ä–æ–≤.")

    await asyncio.gather(*(process_batch(batch) for batch in batches))

    logger.info(f" –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_CSV}")
    return OUTPUT_CSV