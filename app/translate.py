import os
import time
import torch
import json
import logging
import aiofiles
import asyncio
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
import GPUtil

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
    handlers=[logging.FileHandler("translation.log"), logging.StreamHandler()],
)
logger = logging.getLogger()

INPUT_CSV = "/home/aruzhan/products_202503121705.csv"
OUTPUT_CSV = "/home/aruzhan/translated_products.csv"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if torch.cuda.is_available():
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(0)}")
else:
    logger.warning("GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU.")

logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
MODEL_PATH = "/models/t5_translate_model"
tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)  # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ GPU
logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

async def write_to_csv(row):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –û–î–ù–£ —Å—Ç—Ä–æ–∫—É –≤ CSV."""
    async with aiofiles.open(OUTPUT_CSV, mode="a", encoding="utf-8") as f:
        await f.write(",".join(map(str, row)) + "\n")

def translate_batch(texts):
    if not texts:
        logger.warning("–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞!")
        return []

    logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—å...")

    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=256,
            no_repeat_ngram_size=3,
            repetition_penalty=1.2,
            length_penalty=1.1,
            num_beams=7,
            early_stopping=True,
        )

    translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
    logger.info(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ {len(translations)} —Ç–µ–∫—Å—Ç–æ–≤.")
    return translations


async def process_batch(rows, translated_ids):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø–µ—Ä–µ–≤–æ–¥–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞—è —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã."""
    start_time = time.time()
    rows_to_translate = rows[~rows["id"].astype(str).isin(translated_ids)]

    if rows_to_translate.empty:
        logger.info("–í—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤ —ç—Ç–æ–º –±–∞—Ç—á–µ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return

    texts = rows_to_translate["en"].tolist()
    translations = translate_batch(texts)

    for i, (_, row) in enumerate(rows_to_translate.iterrows()):
        translated_text = translations[i]
        logger.info(f" –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ ID {row['id']}: {translated_text} ( {time.time() - start_time:.2f} —Å–µ–∫)")
        await write_to_csv([row['id'], row['en'], translated_text, row['product_id'], row['category_id']])

    elapsed_time = time.time() - start_time
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(rows_to_translate)} —Å—Ç—Ä–æ–∫ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥.")

async def load_existing_translations():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É."""
    if not os.path.exists(OUTPUT_CSV):
        return set()

    try:
        async with aiofiles.open(OUTPUT_CSV, mode="r", encoding="utf-8") as f:
            lines = await f.readlines()
        if len(lines) < 2:
            return set()

        df_translated = pd.read_csv(OUTPUT_CSV, delimiter=",", quotechar='"', on_bad_lines="skip")
        return set(df_translated["id"].astype(str))
    except Exception as e:
        logger.error(f" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        return set()

def parse_json_safe(x):
    try:
        return json.loads(x) if isinstance(x, str) and x.startswith("{") else {}
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ JSON: {e} –¥–ª—è —Å—Ç—Ä–æ–∫–∏: {x}")
        return {}


MIN_CONCURRENCY = 20
MAX_CONCURRENCY = 40
batch_size = 200

async def get_optimal_concurrency():
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø–æ—Ç–æ–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ GPU."""
    try:
        gpus = GPUtil.getGPUs()
        if not gpus:
            logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –∑–∞–≥—Ä—É–∑–∫–µ GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –ø–æ—Ç–æ–∫–æ–≤.")
            return MIN_CONCURRENCY  # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ GPU, —Å—Ç–∞–≤–∏–º –º–∏–Ω–∏–º—É–º

        load = gpus[0].load  # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å GPU (0.0 - 1.0)
        logger.info(f"üéõ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å GPU: {load * 100:.2f}%")
        if load > 0.9:
            return MIN_CONCURRENCY  # –°–∏–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Üí –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç
        elif load < 0.5:
            return MAX_CONCURRENCY  # GPU –Ω–µ–¥–æ–≥—Ä—É–∂–µ–Ω ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç
        else:
            return int((MAX_CONCURRENCY - MIN_CONCURRENCY) * (1 - load) + MIN_CONCURRENCY)
    except Exception as e:
        logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ GPU: {e}")
        return MIN_CONCURRENCY

async def process_csv():
    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f" –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")

    try:
        df = pd.read_csv(INPUT_CSV, on_bad_lines="skip", delimiter=";", quotechar='"')
    except Exception as e:
        logger.error(f" –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {e}")
        return None

    logger.info(f" –ö–æ–ª–æ–Ω–∫–∏ CSV: {df.columns}")

    if "names" not in df.columns:
        raise ValueError(" CSV –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–æ–ª–±—Ü–∞ 'names'")

    df["names"] = df["names"].apply(parse_json_safe)
    df["en"] = df["names"].apply(lambda x: x.get("en", "") if isinstance(x, dict) else "")

    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥...")

    if not os.path.exists(OUTPUT_CSV):
        async with aiofiles.open(OUTPUT_CSV, mode="w", encoding="utf-8") as f:
            await f.write("id,en_name,ru_name,product_id,category_id\n")

    batches = [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]

    logger.info(f"–í—Å–µ–≥–æ {len(batches)} –±–∞—Ç—á–µ–π –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")

    translated_ids = await load_existing_translations()
    logger.info(f" –ù–∞–π–¥–µ–Ω–æ {len(translated_ids)} —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤.")

    concurrency_limit = await get_optimal_concurrency()
    logger.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤: {concurrency_limit}")

    semaphore = asyncio.Semaphore(concurrency_limit)

    async def process_limited(batch, batch_index):
        async with semaphore:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {batch_index+1}/{len(batches)} ({len(batch)} —Å—Ç—Ä–æ–∫)")
            await process_batch(batch, translated_ids)

    tasks = [process_limited(batch, i) for i, batch in enumerate(batches)]
    await asyncio.gather(*tasks)

    logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_CSV}")
    return OUTPUT_CSV
